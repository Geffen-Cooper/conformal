{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = models.efficientnet_b0(weights='DEFAULT').to('cuda').eval()\n",
    "student = models.shufflenet_v2_x0_5(weights='DEFAULT').to('cuda').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # first reinitialize the layer before classification to match the teacher feature dimensions\n",
    "    student.conv5[0] = torch.nn.Conv2d(student.conv5[0].in_channels,teacher.classifier[1].in_features,kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    student.conv5[1] = torch.nn.BatchNorm2d(teacher.classifier[1].in_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\n",
    "\n",
    "    # next create a new fc layer to match the teacher dimension\n",
    "    student.fc = torch.nn.Linear(teacher.classifier[1].in_features,teacher.classifier[1].out_features)\n",
    "\n",
    "    # finally copy the teacher fc parameters\n",
    "    student.fc.weight[:,:] = teacher.classifier[1].weight[:,:]\n",
    "    student.fc.bias[:] = teacher.classifier[1].bias[:]\n",
    "\n",
    "    # freeze the classification layer\n",
    "    for param in student.fc.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    student.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 13:33:35.626753: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-27 13:33:35.759191: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-27 13:33:36.365110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1281167 (0%)] train loss: 7.200, lr: 0.00400000\n",
      "Train Epoch: 0 [51200/1281167 (4%)] train loss: 4.650, lr: 0.00400000\n",
      "Train Epoch: 0 [102400/1281167 (8%)] train loss: 4.277, lr: 0.00398000\n",
      "Train Epoch: 0 [153600/1281167 (12%)] train loss: 3.504, lr: 0.00396010\n",
      "Train Epoch: 0 [204800/1281167 (16%)] train loss: 3.465, lr: 0.00394030\n",
      "Train Epoch: 0 [256000/1281167 (20%)] train loss: 3.473, lr: 0.00392060\n",
      "Train Epoch: 0 [307200/1281167 (24%)] train loss: 2.986, lr: 0.00390100\n",
      "Train Epoch: 0 [358400/1281167 (28%)] train loss: 3.011, lr: 0.00388149\n",
      "Train Epoch: 0 [409600/1281167 (32%)] train loss: 2.918, lr: 0.00386208\n",
      "Train Epoch: 0 [460800/1281167 (36%)] train loss: 3.220, lr: 0.00384277\n",
      "Train Epoch: 0 [512000/1281167 (40%)] train loss: 3.184, lr: 0.00382356\n",
      "Train Epoch: 0 [563200/1281167 (44%)] train loss: 2.823, lr: 0.00380444\n",
      "Train Epoch: 0 [614400/1281167 (48%)] train loss: 2.750, lr: 0.00378542\n",
      "Train Epoch: 0 [665600/1281167 (52%)] train loss: 2.800, lr: 0.00376649\n",
      "Train Epoch: 0 [716800/1281167 (56%)] train loss: 2.687, lr: 0.00374766\n",
      "Train Epoch: 0 [768000/1281167 (60%)] train loss: 2.729, lr: 0.00372892\n",
      "Train Epoch: 0 [819200/1281167 (64%)] train loss: 3.335, lr: 0.00371028\n",
      "Train Epoch: 0 [870400/1281167 (68%)] train loss: 2.778, lr: 0.00369172\n",
      "Train Epoch: 0 [921600/1281167 (72%)] train loss: 2.587, lr: 0.00367327\n",
      "Train Epoch: 0 [972800/1281167 (76%)] train loss: 3.019, lr: 0.00365490\n",
      "Train Epoch: 0 [1024000/1281167 (80%)] train loss: 3.122, lr: 0.00363663\n",
      "Train Epoch: 0 [1075200/1281167 (84%)] train loss: 2.876, lr: 0.00361844\n",
      "Train Epoch: 0 [1126400/1281167 (88%)] train loss: 2.282, lr: 0.00360035\n",
      "Train Epoch: 0 [1177600/1281167 (92%)] train loss: 2.875, lr: 0.00358235\n",
      "Train Epoch: 0 [1228800/1281167 (96%)] train loss: 2.752, lr: 0.00356444\n",
      "Train Epoch: 0 [1280000/1281167 (100%)] train loss: 2.593, lr: 0.00354661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:10<00:00, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [300270/1281167 (100%)] train loss: 3.235, val loss: 2.713, val acc: 0.404, top5: 0.669, lr: 0.00352888\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 0, val accuracy: 0.40392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1281167 (0%)] train loss: 2.234, lr: 0.00352888\n",
      "Train Epoch: 1 [51200/1281167 (4%)] train loss: 3.031, lr: 0.00352888\n",
      "Train Epoch: 1 [102400/1281167 (8%)] train loss: 2.628, lr: 0.00175562\n",
      "Train Epoch: 1 [153600/1281167 (12%)] train loss: 2.742, lr: 0.00174684\n",
      "Train Epoch: 1 [204800/1281167 (16%)] train loss: 2.671, lr: 0.00173811\n",
      "Train Epoch: 1 [256000/1281167 (20%)] train loss: 3.132, lr: 0.00172942\n",
      "Train Epoch: 1 [307200/1281167 (24%)] train loss: 2.300, lr: 0.00172077\n",
      "Train Epoch: 1 [358400/1281167 (28%)] train loss: 3.214, lr: 0.00171216\n",
      "Train Epoch: 1 [409600/1281167 (32%)] train loss: 2.964, lr: 0.00170360\n",
      "Train Epoch: 1 [460800/1281167 (36%)] train loss: 2.468, lr: 0.00169509\n",
      "Train Epoch: 1 [512000/1281167 (40%)] train loss: 2.569, lr: 0.00168661\n",
      "Train Epoch: 1 [563200/1281167 (44%)] train loss: 2.460, lr: 0.00167818\n",
      "Train Epoch: 1 [614400/1281167 (48%)] train loss: 2.506, lr: 0.00166979\n",
      "Train Epoch: 1 [665600/1281167 (52%)] train loss: 2.620, lr: 0.00166144\n",
      "Train Epoch: 1 [716800/1281167 (56%)] train loss: 3.044, lr: 0.00165313\n",
      "Train Epoch: 1 [768000/1281167 (60%)] train loss: 2.308, lr: 0.00164486\n",
      "Train Epoch: 1 [819200/1281167 (64%)] train loss: 2.341, lr: 0.00163664\n",
      "Train Epoch: 1 [870400/1281167 (68%)] train loss: 2.562, lr: 0.00162846\n",
      "Train Epoch: 1 [921600/1281167 (72%)] train loss: 2.400, lr: 0.00162031\n",
      "Train Epoch: 1 [972800/1281167 (76%)] train loss: 2.359, lr: 0.00161221\n",
      "Train Epoch: 1 [1024000/1281167 (80%)] train loss: 2.708, lr: 0.00160415\n",
      "Train Epoch: 1 [1075200/1281167 (84%)] train loss: 2.739, lr: 0.00159613\n",
      "Train Epoch: 1 [1126400/1281167 (88%)] train loss: 2.598, lr: 0.00158815\n",
      "Train Epoch: 1 [1177600/1281167 (92%)] train loss: 2.023, lr: 0.00158021\n",
      "Train Epoch: 1 [1228800/1281167 (96%)] train loss: 2.667, lr: 0.00157231\n",
      "Train Epoch: 1 [1280000/1281167 (100%)] train loss: 2.309, lr: 0.00156445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:10<00:00, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [300270/1281167 (100%)] train loss: 1.980, val loss: 2.564, val acc: 0.433, top5: 0.693, lr: 0.00155663\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 1, val accuracy: 0.43348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/1281167 (0%)] train loss: 2.169, lr: 0.00155663\n",
      "Train Epoch: 2 [51200/1281167 (4%)] train loss: 2.182, lr: 0.00155663\n",
      "Train Epoch: 2 [102400/1281167 (8%)] train loss: 2.222, lr: 0.00077442\n",
      "Train Epoch: 2 [153600/1281167 (12%)] train loss: 2.144, lr: 0.00077055\n",
      "Train Epoch: 2 [204800/1281167 (16%)] train loss: 1.952, lr: 0.00076670\n",
      "Train Epoch: 2 [256000/1281167 (20%)] train loss: 1.955, lr: 0.00076286\n",
      "Train Epoch: 2 [307200/1281167 (24%)] train loss: 2.234, lr: 0.00075905\n",
      "Train Epoch: 2 [358400/1281167 (28%)] train loss: 1.714, lr: 0.00075525\n",
      "Train Epoch: 2 [409600/1281167 (32%)] train loss: 2.611, lr: 0.00075148\n",
      "Train Epoch: 2 [460800/1281167 (36%)] train loss: 1.798, lr: 0.00074772\n",
      "Train Epoch: 2 [512000/1281167 (40%)] train loss: 2.180, lr: 0.00074398\n",
      "Train Epoch: 2 [563200/1281167 (44%)] train loss: 1.912, lr: 0.00074026\n",
      "Train Epoch: 2 [614400/1281167 (48%)] train loss: 1.757, lr: 0.00073656\n",
      "Train Epoch: 2 [665600/1281167 (52%)] train loss: 2.237, lr: 0.00073288\n",
      "Train Epoch: 2 [716800/1281167 (56%)] train loss: 2.598, lr: 0.00072921\n",
      "Train Epoch: 2 [768000/1281167 (60%)] train loss: 2.273, lr: 0.00072557\n",
      "Train Epoch: 2 [819200/1281167 (64%)] train loss: 2.705, lr: 0.00072194\n",
      "Train Epoch: 2 [870400/1281167 (68%)] train loss: 2.063, lr: 0.00071833\n",
      "Train Epoch: 2 [921600/1281167 (72%)] train loss: 2.046, lr: 0.00071474\n",
      "Train Epoch: 2 [972800/1281167 (76%)] train loss: 2.162, lr: 0.00071116\n",
      "Train Epoch: 2 [1024000/1281167 (80%)] train loss: 2.099, lr: 0.00070761\n",
      "Train Epoch: 2 [1075200/1281167 (84%)] train loss: 2.133, lr: 0.00070407\n",
      "Train Epoch: 2 [1126400/1281167 (88%)] train loss: 2.401, lr: 0.00070055\n",
      "Train Epoch: 2 [1177600/1281167 (92%)] train loss: 1.990, lr: 0.00069705\n",
      "Train Epoch: 2 [1228800/1281167 (96%)] train loss: 1.728, lr: 0.00069356\n",
      "Train Epoch: 2 [1280000/1281167 (100%)] train loss: 1.952, lr: 0.00069009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:10<00:00, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [300270/1281167 (100%)] train loss: 1.862, val loss: 2.341, val acc: 0.477, top5: 0.726, lr: 0.00068664\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 2, val accuracy: 0.47742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/1281167 (0%)] train loss: 2.482, lr: 0.00068664\n",
      "Train Epoch: 3 [51200/1281167 (4%)] train loss: 2.088, lr: 0.00068664\n",
      "Train Epoch: 3 [102400/1281167 (8%)] train loss: 2.283, lr: 0.00034160\n",
      "Train Epoch: 3 [153600/1281167 (12%)] train loss: 2.034, lr: 0.00033990\n",
      "Train Epoch: 3 [204800/1281167 (16%)] train loss: 1.916, lr: 0.00033820\n",
      "Train Epoch: 3 [256000/1281167 (20%)] train loss: 1.777, lr: 0.00033651\n",
      "Train Epoch: 3 [307200/1281167 (24%)] train loss: 2.054, lr: 0.00033482\n",
      "Train Epoch: 3 [358400/1281167 (28%)] train loss: 1.986, lr: 0.00033315\n",
      "Train Epoch: 3 [409600/1281167 (32%)] train loss: 2.563, lr: 0.00033148\n",
      "Train Epoch: 3 [460800/1281167 (36%)] train loss: 2.270, lr: 0.00032983\n",
      "Train Epoch: 3 [512000/1281167 (40%)] train loss: 1.987, lr: 0.00032818\n",
      "Train Epoch: 3 [563200/1281167 (44%)] train loss: 2.300, lr: 0.00032654\n",
      "Train Epoch: 3 [614400/1281167 (48%)] train loss: 2.122, lr: 0.00032490\n",
      "Train Epoch: 3 [665600/1281167 (52%)] train loss: 1.766, lr: 0.00032328\n",
      "Train Epoch: 3 [716800/1281167 (56%)] train loss: 2.109, lr: 0.00032166\n",
      "Train Epoch: 3 [768000/1281167 (60%)] train loss: 1.886, lr: 0.00032005\n",
      "Train Epoch: 3 [819200/1281167 (64%)] train loss: 1.888, lr: 0.00031845\n",
      "Train Epoch: 3 [870400/1281167 (68%)] train loss: 2.366, lr: 0.00031686\n",
      "Train Epoch: 3 [921600/1281167 (72%)] train loss: 1.562, lr: 0.00031528\n",
      "Train Epoch: 3 [972800/1281167 (76%)] train loss: 1.567, lr: 0.00031370\n",
      "Train Epoch: 3 [1024000/1281167 (80%)] train loss: 2.103, lr: 0.00031213\n",
      "Train Epoch: 3 [1075200/1281167 (84%)] train loss: 1.772, lr: 0.00031057\n",
      "Train Epoch: 3 [1126400/1281167 (88%)] train loss: 1.753, lr: 0.00030902\n",
      "Train Epoch: 3 [1177600/1281167 (92%)] train loss: 2.153, lr: 0.00030747\n",
      "Train Epoch: 3 [1228800/1281167 (96%)] train loss: 2.192, lr: 0.00030594\n",
      "Train Epoch: 3 [1280000/1281167 (100%)] train loss: 2.137, lr: 0.00030441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:11<00:00, 11.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [300270/1281167 (100%)] train loss: 1.666, val loss: 2.254, val acc: 0.494, top5: 0.740, lr: 0.00030289\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 3, val accuracy: 0.49422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/1281167 (0%)] train loss: 1.960, lr: 0.00030289\n",
      "Train Epoch: 4 [51200/1281167 (4%)] train loss: 2.048, lr: 0.00030289\n",
      "Train Epoch: 4 [102400/1281167 (8%)] train loss: 1.919, lr: 0.00015069\n",
      "Train Epoch: 4 [153600/1281167 (12%)] train loss: 1.831, lr: 0.00014993\n",
      "Train Epoch: 4 [204800/1281167 (16%)] train loss: 2.310, lr: 0.00014918\n",
      "Train Epoch: 4 [256000/1281167 (20%)] train loss: 2.062, lr: 0.00014844\n",
      "Train Epoch: 4 [307200/1281167 (24%)] train loss: 1.723, lr: 0.00014769\n",
      "Train Epoch: 4 [358400/1281167 (28%)] train loss: 1.985, lr: 0.00014696\n",
      "Train Epoch: 4 [409600/1281167 (32%)] train loss: 2.410, lr: 0.00014622\n",
      "Train Epoch: 4 [460800/1281167 (36%)] train loss: 1.460, lr: 0.00014549\n",
      "Train Epoch: 4 [512000/1281167 (40%)] train loss: 2.094, lr: 0.00014476\n",
      "Train Epoch: 4 [563200/1281167 (44%)] train loss: 1.800, lr: 0.00014404\n",
      "Train Epoch: 4 [614400/1281167 (48%)] train loss: 1.798, lr: 0.00014332\n",
      "Train Epoch: 4 [665600/1281167 (52%)] train loss: 1.903, lr: 0.00014260\n",
      "Train Epoch: 4 [716800/1281167 (56%)] train loss: 1.787, lr: 0.00014189\n",
      "Train Epoch: 4 [768000/1281167 (60%)] train loss: 1.648, lr: 0.00014118\n",
      "Train Epoch: 4 [819200/1281167 (64%)] train loss: 1.540, lr: 0.00014047\n",
      "Train Epoch: 4 [870400/1281167 (68%)] train loss: 1.669, lr: 0.00013977\n",
      "Train Epoch: 4 [921600/1281167 (72%)] train loss: 1.781, lr: 0.00013907\n",
      "Train Epoch: 4 [972800/1281167 (76%)] train loss: 1.892, lr: 0.00013838\n",
      "Train Epoch: 4 [1024000/1281167 (80%)] train loss: 1.985, lr: 0.00013768\n",
      "Train Epoch: 4 [1075200/1281167 (84%)] train loss: 2.161, lr: 0.00013700\n",
      "Train Epoch: 4 [1126400/1281167 (88%)] train loss: 1.563, lr: 0.00013631\n",
      "Train Epoch: 4 [1177600/1281167 (92%)] train loss: 2.041, lr: 0.00013563\n",
      "Train Epoch: 4 [1228800/1281167 (96%)] train loss: 1.714, lr: 0.00013495\n",
      "Train Epoch: 4 [1280000/1281167 (100%)] train loss: 2.702, lr: 0.00013428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:10<00:00, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [300270/1281167 (100%)] train loss: 2.009, val loss: 2.214, val acc: 0.502, top5: 0.747, lr: 0.00013361\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 4, val accuracy: 0.50168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/1281167 (0%)] train loss: 1.576, lr: 0.00013361\n",
      "Train Epoch: 5 [51200/1281167 (4%)] train loss: 2.179, lr: 0.00013361\n",
      "Train Epoch: 5 [102400/1281167 (8%)] train loss: 1.686, lr: 0.00006647\n",
      "Train Epoch: 5 [153600/1281167 (12%)] train loss: 2.054, lr: 0.00006614\n",
      "Train Epoch: 5 [204800/1281167 (16%)] train loss: 1.758, lr: 0.00006581\n",
      "Train Epoch: 5 [256000/1281167 (20%)] train loss: 2.280, lr: 0.00006548\n",
      "Train Epoch: 5 [307200/1281167 (24%)] train loss: 2.188, lr: 0.00006515\n",
      "Train Epoch: 5 [358400/1281167 (28%)] train loss: 1.565, lr: 0.00006482\n",
      "Train Epoch: 5 [409600/1281167 (32%)] train loss: 1.554, lr: 0.00006450\n",
      "Train Epoch: 5 [460800/1281167 (36%)] train loss: 2.141, lr: 0.00006418\n",
      "Train Epoch: 5 [512000/1281167 (40%)] train loss: 1.842, lr: 0.00006386\n",
      "Train Epoch: 5 [563200/1281167 (44%)] train loss: 2.538, lr: 0.00006354\n",
      "Train Epoch: 5 [614400/1281167 (48%)] train loss: 1.774, lr: 0.00006322\n",
      "Train Epoch: 5 [665600/1281167 (52%)] train loss: 1.737, lr: 0.00006290\n",
      "Train Epoch: 5 [716800/1281167 (56%)] train loss: 1.957, lr: 0.00006259\n",
      "Train Epoch: 5 [768000/1281167 (60%)] train loss: 1.760, lr: 0.00006228\n",
      "Train Epoch: 5 [819200/1281167 (64%)] train loss: 1.507, lr: 0.00006196\n",
      "Train Epoch: 5 [870400/1281167 (68%)] train loss: 1.528, lr: 0.00006165\n",
      "Train Epoch: 5 [921600/1281167 (72%)] train loss: 1.513, lr: 0.00006135\n",
      "Train Epoch: 5 [972800/1281167 (76%)] train loss: 2.196, lr: 0.00006104\n",
      "Train Epoch: 5 [1024000/1281167 (80%)] train loss: 1.750, lr: 0.00006073\n",
      "Train Epoch: 5 [1075200/1281167 (84%)] train loss: 1.715, lr: 0.00006043\n",
      "Train Epoch: 5 [1126400/1281167 (88%)] train loss: 1.763, lr: 0.00006013\n",
      "Train Epoch: 5 [1177600/1281167 (92%)] train loss: 1.840, lr: 0.00005983\n",
      "Train Epoch: 5 [1228800/1281167 (96%)] train loss: 1.723, lr: 0.00005953\n",
      "Train Epoch: 5 [1280000/1281167 (100%)] train loss: 2.062, lr: 0.00005923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:11<00:00, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [300270/1281167 (100%)] train loss: 2.445, val loss: 2.199, val acc: 0.507, top5: 0.748, lr: 0.00005893\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 5, val accuracy: 0.50692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/1281167 (0%)] train loss: 1.667, lr: 0.00005893\n",
      "Train Epoch: 6 [51200/1281167 (4%)] train loss: 1.642, lr: 0.00005893\n",
      "Train Epoch: 6 [102400/1281167 (8%)] train loss: 1.838, lr: 0.00002932\n",
      "Train Epoch: 6 [153600/1281167 (12%)] train loss: 1.887, lr: 0.00002917\n",
      "Train Epoch: 6 [204800/1281167 (16%)] train loss: 1.575, lr: 0.00002903\n",
      "Train Epoch: 6 [256000/1281167 (20%)] train loss: 1.856, lr: 0.00002888\n",
      "Train Epoch: 6 [307200/1281167 (24%)] train loss: 2.175, lr: 0.00002874\n",
      "Train Epoch: 6 [358400/1281167 (28%)] train loss: 1.987, lr: 0.00002859\n",
      "Train Epoch: 6 [409600/1281167 (32%)] train loss: 1.554, lr: 0.00002845\n",
      "Train Epoch: 6 [460800/1281167 (36%)] train loss: 1.389, lr: 0.00002831\n",
      "Train Epoch: 6 [512000/1281167 (40%)] train loss: 1.255, lr: 0.00002817\n",
      "Train Epoch: 6 [563200/1281167 (44%)] train loss: 1.642, lr: 0.00002803\n",
      "Train Epoch: 6 [614400/1281167 (48%)] train loss: 1.586, lr: 0.00002789\n",
      "Train Epoch: 6 [665600/1281167 (52%)] train loss: 1.793, lr: 0.00002775\n",
      "Train Epoch: 6 [716800/1281167 (56%)] train loss: 2.039, lr: 0.00002761\n",
      "Train Epoch: 6 [768000/1281167 (60%)] train loss: 1.681, lr: 0.00002747\n",
      "Train Epoch: 6 [819200/1281167 (64%)] train loss: 2.156, lr: 0.00002733\n",
      "Train Epoch: 6 [870400/1281167 (68%)] train loss: 1.749, lr: 0.00002720\n",
      "Train Epoch: 6 [921600/1281167 (72%)] train loss: 1.702, lr: 0.00002706\n",
      "Train Epoch: 6 [972800/1281167 (76%)] train loss: 2.501, lr: 0.00002693\n",
      "Train Epoch: 6 [1024000/1281167 (80%)] train loss: 1.225, lr: 0.00002679\n",
      "Train Epoch: 6 [1075200/1281167 (84%)] train loss: 1.824, lr: 0.00002666\n",
      "Train Epoch: 6 [1126400/1281167 (88%)] train loss: 2.295, lr: 0.00002652\n",
      "Train Epoch: 6 [1177600/1281167 (92%)] train loss: 1.840, lr: 0.00002639\n",
      "Train Epoch: 6 [1228800/1281167 (96%)] train loss: 2.102, lr: 0.00002626\n",
      "Train Epoch: 6 [1280000/1281167 (100%)] train loss: 1.629, lr: 0.00002613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:10<00:00, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [300270/1281167 (100%)] train loss: 2.468, val loss: 2.192, val acc: 0.509, top5: 0.749, lr: 0.00002600\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 6, val accuracy: 0.50882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/1281167 (0%)] train loss: 1.933, lr: 0.00002600\n",
      "Train Epoch: 7 [51200/1281167 (4%)] train loss: 1.621, lr: 0.00002600\n",
      "Train Epoch: 7 [102400/1281167 (8%)] train loss: 1.675, lr: 0.00001293\n",
      "Train Epoch: 7 [153600/1281167 (12%)] train loss: 1.742, lr: 0.00001287\n",
      "Train Epoch: 7 [204800/1281167 (16%)] train loss: 1.459, lr: 0.00001280\n",
      "Train Epoch: 7 [256000/1281167 (20%)] train loss: 2.068, lr: 0.00001274\n",
      "Train Epoch: 7 [307200/1281167 (24%)] train loss: 1.828, lr: 0.00001268\n",
      "Train Epoch: 7 [358400/1281167 (28%)] train loss: 1.985, lr: 0.00001261\n",
      "Train Epoch: 7 [409600/1281167 (32%)] train loss: 1.917, lr: 0.00001255\n",
      "Train Epoch: 7 [460800/1281167 (36%)] train loss: 1.669, lr: 0.00001249\n",
      "Train Epoch: 7 [512000/1281167 (40%)] train loss: 1.567, lr: 0.00001243\n",
      "Train Epoch: 7 [563200/1281167 (44%)] train loss: 1.893, lr: 0.00001236\n",
      "Train Epoch: 7 [614400/1281167 (48%)] train loss: 1.891, lr: 0.00001230\n",
      "Train Epoch: 7 [665600/1281167 (52%)] train loss: 2.224, lr: 0.00001224\n",
      "Train Epoch: 7 [716800/1281167 (56%)] train loss: 2.345, lr: 0.00001218\n",
      "Train Epoch: 7 [768000/1281167 (60%)] train loss: 1.756, lr: 0.00001212\n",
      "Train Epoch: 7 [819200/1281167 (64%)] train loss: 2.579, lr: 0.00001206\n",
      "Train Epoch: 7 [870400/1281167 (68%)] train loss: 1.598, lr: 0.00001200\n",
      "Train Epoch: 7 [921600/1281167 (72%)] train loss: 1.561, lr: 0.00001194\n",
      "Train Epoch: 7 [972800/1281167 (76%)] train loss: 1.645, lr: 0.00001188\n",
      "Train Epoch: 7 [1024000/1281167 (80%)] train loss: 2.210, lr: 0.00001182\n",
      "Train Epoch: 7 [1075200/1281167 (84%)] train loss: 1.986, lr: 0.00001176\n",
      "Train Epoch: 7 [1126400/1281167 (88%)] train loss: 1.492, lr: 0.00001170\n",
      "Train Epoch: 7 [1177600/1281167 (92%)] train loss: 2.063, lr: 0.00001164\n",
      "Train Epoch: 7 [1228800/1281167 (96%)] train loss: 1.744, lr: 0.00001158\n",
      "Train Epoch: 7 [1280000/1281167 (100%)] train loss: 2.191, lr: 0.00001153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:10<00:00, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [300270/1281167 (100%)] train loss: 1.489, val loss: 2.186, val acc: 0.511, top5: 0.751, lr: 0.00001147\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 7, val accuracy: 0.51142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/1281167 (0%)] train loss: 2.484, lr: 0.00001147\n",
      "Train Epoch: 8 [51200/1281167 (4%)] train loss: 1.477, lr: 0.00001147\n",
      "Train Epoch: 8 [102400/1281167 (8%)] train loss: 1.885, lr: 0.00000571\n",
      "Train Epoch: 8 [153600/1281167 (12%)] train loss: 1.619, lr: 0.00000568\n",
      "Train Epoch: 8 [204800/1281167 (16%)] train loss: 1.880, lr: 0.00000565\n",
      "Train Epoch: 8 [256000/1281167 (20%)] train loss: 2.271, lr: 0.00000562\n",
      "Train Epoch: 8 [307200/1281167 (24%)] train loss: 2.570, lr: 0.00000559\n",
      "Train Epoch: 8 [358400/1281167 (28%)] train loss: 2.049, lr: 0.00000556\n",
      "Train Epoch: 8 [409600/1281167 (32%)] train loss: 1.955, lr: 0.00000554\n",
      "Train Epoch: 8 [460800/1281167 (36%)] train loss: 1.774, lr: 0.00000551\n",
      "Train Epoch: 8 [512000/1281167 (40%)] train loss: 2.018, lr: 0.00000548\n",
      "Train Epoch: 8 [563200/1281167 (44%)] train loss: 1.558, lr: 0.00000545\n",
      "Train Epoch: 8 [614400/1281167 (48%)] train loss: 1.775, lr: 0.00000543\n",
      "Train Epoch: 8 [665600/1281167 (52%)] train loss: 1.770, lr: 0.00000540\n",
      "Train Epoch: 8 [716800/1281167 (56%)] train loss: 2.038, lr: 0.00000537\n",
      "Train Epoch: 8 [768000/1281167 (60%)] train loss: 1.956, lr: 0.00000535\n",
      "Train Epoch: 8 [819200/1281167 (64%)] train loss: 2.028, lr: 0.00000532\n",
      "Train Epoch: 8 [870400/1281167 (68%)] train loss: 1.850, lr: 0.00000529\n",
      "Train Epoch: 8 [921600/1281167 (72%)] train loss: 1.977, lr: 0.00000527\n",
      "Train Epoch: 8 [972800/1281167 (76%)] train loss: 1.708, lr: 0.00000524\n",
      "Train Epoch: 8 [1024000/1281167 (80%)] train loss: 1.510, lr: 0.00000521\n",
      "Train Epoch: 8 [1075200/1281167 (84%)] train loss: 1.692, lr: 0.00000519\n",
      "Train Epoch: 8 [1126400/1281167 (88%)] train loss: 2.187, lr: 0.00000516\n",
      "Train Epoch: 8 [1177600/1281167 (92%)] train loss: 1.746, lr: 0.00000514\n",
      "Train Epoch: 8 [1228800/1281167 (96%)] train loss: 2.148, lr: 0.00000511\n",
      "Train Epoch: 8 [1280000/1281167 (100%)] train loss: 1.946, lr: 0.00000508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:12<00:00, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [300270/1281167 (100%)] train loss: 2.613, val loss: 2.185, val acc: 0.511, top5: 0.753, lr: 0.00000506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/1281167 (0%)] train loss: 1.797, lr: 0.00000506\n",
      "Train Epoch: 9 [51200/1281167 (4%)] train loss: 1.555, lr: 0.00000506\n",
      "Train Epoch: 9 [102400/1281167 (8%)] train loss: 1.882, lr: 0.00000252\n",
      "Train Epoch: 9 [153600/1281167 (12%)] train loss: 1.766, lr: 0.00000250\n",
      "Train Epoch: 9 [204800/1281167 (16%)] train loss: 1.291, lr: 0.00000249\n",
      "Train Epoch: 9 [256000/1281167 (20%)] train loss: 1.908, lr: 0.00000248\n",
      "Train Epoch: 9 [307200/1281167 (24%)] train loss: 1.706, lr: 0.00000247\n",
      "Train Epoch: 9 [358400/1281167 (28%)] train loss: 2.092, lr: 0.00000245\n",
      "Train Epoch: 9 [409600/1281167 (32%)] train loss: 1.375, lr: 0.00000244\n",
      "Train Epoch: 9 [460800/1281167 (36%)] train loss: 1.972, lr: 0.00000243\n",
      "Train Epoch: 9 [512000/1281167 (40%)] train loss: 1.638, lr: 0.00000242\n",
      "Train Epoch: 9 [563200/1281167 (44%)] train loss: 1.984, lr: 0.00000241\n",
      "Train Epoch: 9 [614400/1281167 (48%)] train loss: 1.637, lr: 0.00000239\n",
      "Train Epoch: 9 [665600/1281167 (52%)] train loss: 1.619, lr: 0.00000238\n",
      "Train Epoch: 9 [716800/1281167 (56%)] train loss: 1.740, lr: 0.00000237\n",
      "Train Epoch: 9 [768000/1281167 (60%)] train loss: 2.040, lr: 0.00000236\n",
      "Train Epoch: 9 [819200/1281167 (64%)] train loss: 1.968, lr: 0.00000235\n",
      "Train Epoch: 9 [870400/1281167 (68%)] train loss: 1.752, lr: 0.00000233\n",
      "Train Epoch: 9 [921600/1281167 (72%)] train loss: 1.821, lr: 0.00000232\n",
      "Train Epoch: 9 [972800/1281167 (76%)] train loss: 1.550, lr: 0.00000231\n",
      "Train Epoch: 9 [1024000/1281167 (80%)] train loss: 1.206, lr: 0.00000230\n",
      "Train Epoch: 9 [1075200/1281167 (84%)] train loss: 1.783, lr: 0.00000229\n",
      "Train Epoch: 9 [1126400/1281167 (88%)] train loss: 1.393, lr: 0.00000228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_loader, val_loader \u001b[39m=\u001b[39m load_imagenet(\u001b[39m64\u001b[39m,\u001b[39m12345\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train(student,train_loader,val_loader,\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m,lr\u001b[39m=\u001b[39;49m\u001b[39m0.004\u001b[39;49m,epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msn_frozen_headval\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Projects/conformal/adaptation/train.py:39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, device, lr, epochs, grad_accum, log_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m batch_iter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     40\u001b[0m         \u001b[39m# print(\"train, num samples:\", len(target))\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[39m# Big Forward\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m         \u001b[39mif\u001b[39;00m grad_accum \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hs/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/hs/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hs/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1272\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1271\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1272\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1273\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1274\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/hs/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hs/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/hs/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = load_imagenet(64,12345)\n",
    "train(student,train_loader,val_loader,'cuda',lr=0.004,epochs=100,log_name=\"sn_frozen_headval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = load_imagenet(128,1234,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.142319898206824e-06]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"best_batch_i288264sn_frozen_head_long1682578354.1065493.pth\")['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffleNetV2(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (stage2): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(192, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = models.shufflenet_v2_x0_5(weights='DEFAULT').to('cuda')\n",
    "with torch.no_grad():\n",
    "    # first reinitialize the layer before classification to match the teacher feature dimensions\n",
    "    student.conv5[0] = torch.nn.Conv2d(student.conv5[0].in_channels,teacher.classifier[1].in_features,kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    student.conv5[1] = torch.nn.BatchNorm2d(teacher.classifier[1].in_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\n",
    "\n",
    "    # next create a new fc layer to match the teacher dimension\n",
    "    student.fc = torch.nn.Linear(teacher.classifier[1].in_features,teacher.classifier[1].out_features)\n",
    "\n",
    "    # freeze the classification layer\n",
    "    for param in student.fc.parameters():\n",
    "        param.requires_grad = False\n",
    "student.load_state_dict(torch.load(\"best_batch_i288264sn_frozen_head_long1682578354.1065493.pth\")['model_state_dict'])\n",
    "student.to('cuda')\n",
    "student.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [04:39<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "acc = validate(student,test_loader,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50772, 2.187045480436681, 0.75248)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71152ab9c07ce901c8cf95cbd74fadea2c31d5b816d92473f31e695d403a1560"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
